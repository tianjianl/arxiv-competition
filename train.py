#!/usr/bin/env python
# coding: utf-8

# # è®ºæ–‡å¼•ç”¨ç½‘ç»œèŠ‚ç‚¹åˆ†ç±»æ¯”èµ›
# 
# ## èµ›é¢˜ä»‹ç»
# 
# 
# å›¾ç¥ç»ç½‘ç»œï¼ˆGraph Neural Networkï¼‰æ˜¯ä¸€ç§ä¸“é—¨å¤„ç†å›¾ç»“æ„æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œç›®å‰è¢«å¹¿æ³›åº”ç”¨äºæ¨èç³»ç»Ÿã€é‡‘èé£æ§ã€ç”Ÿç‰©è®¡ç®—ä¸­ã€‚å›¾ç¥ç»ç½‘ç»œçš„ç»å…¸é—®é¢˜ä¸»è¦æœ‰ä¸‰ç§ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹åˆ†ç±»ã€è¿æ¥é¢„æµ‹å’Œå›¾åˆ†ç±»ä¸‰ç§ã€‚æœ¬æ¬¡æ¯”èµ›æ˜¯å›¾ç¥ç»ç½‘ç»œ7æ—¥æ‰“å¡è¯¾ç¨‹çš„å¤§ä½œä¸šï¼Œä¸»è¦è®©åŒå­¦ä»¬ç†Ÿæ‚‰å¦‚ä½•å›¾ç¥ç»ç½‘ç»œå¤„ç†èŠ‚ç‚¹åˆ†ç±»é—®é¢˜ã€‚
# 
# æ•°æ®é›†ä¸ºè®ºæ–‡å¼•ç”¨ç½‘ç»œï¼Œå›¾ç”±å¤§é‡çš„å­¦æœ¯è®ºæ–‡ç»„æˆï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„è¾¹æ˜¯è®ºæ–‡çš„å¼•ç”¨å…³ç³»ï¼Œæ¯ä¸€ä¸ªèŠ‚ç‚¹æä¾›ç®€å•çš„è¯å‘é‡ç»„åˆçš„èŠ‚ç‚¹ç‰¹å¾ã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯ç»™æ¯ç¯‡è®ºæ–‡æ¨æ–­å‡ºå®ƒçš„è®ºæ–‡ç±»åˆ«ã€‚
# 
# 
# 
# 

# ## è¿è¡Œæ–¹å¼
# æœ¬æ¬¡åŸºçº¿åŸºäºé£æ¡¨PaddlePaddle 2.2.0ç‰ˆæœ¬ï¼Œè‹¥æœ¬åœ°è¿è¡Œåˆ™å¯èƒ½éœ€è¦é¢å¤–å®‰è£…pglã€easydictã€pandasç­‰æ¨¡å—ã€‚
# 
# ## æœ¬åœ°è¿è¡Œ
# ä¸‹è½½å·¦ä¾§æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰pyæ–‡ä»¶ï¼ˆåŒ…æ‹¬build_model.py, model.pyï¼‰,ä»¥åŠworkç›®å½•ï¼Œç„¶ååœ¨å³ä¸Šè§’â€œæ–‡ä»¶â€->â€œå¯¼å‡ºNotebookåˆ°pyâ€ï¼Œè¿™æ ·å¯ä»¥ä¿è¯ä»£ç æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼‰ï¼Œæ‰§è¡Œå¯¼å‡ºçš„pyæ–‡ä»¶å³å¯ã€‚å®Œæˆåä¸‹è½½submission.csvæäº¤ç»“æœå³å¯ã€‚
# 
# ## AI Studio (Notebook)è¿è¡Œ
# ä¾æ¬¡è¿è¡Œä¸‹æ–¹çš„cellï¼Œå®Œæˆåä¸‹è½½submission.csvæäº¤ç»“æœå³å¯ã€‚è‹¥è¿è¡Œæ—¶ä¿®æ”¹äº†cellï¼Œæ¨èåœ¨å³ä¸Šè§’é‡å¯æ‰§è¡Œå™¨åå†ä»¥æ­¤è¿è¡Œï¼Œé¿å…å› å†…å­˜æœªæ¸…ç©ºè€Œäº§ç”ŸæŠ¥é”™ã€‚ Tipsï¼šè‹¥ä¿®æ”¹äº†å·¦ä¾§æ–‡ä»¶å¤¹ä¸­æ•°æ®ï¼Œä¹Ÿéœ€è¦é‡å¯æ‰§è¡Œå™¨åæ‰ä¼šåŠ è½½æ–°æ–‡ä»¶ã€‚

# ## ä»£ç æ•´ä½“é€»è¾‘
# 
# 1. è¯»å–æä¾›çš„æ•°æ®é›†ï¼ŒåŒ…å«æ„å›¾ä»¥åŠè¯»å–èŠ‚ç‚¹ç‰¹å¾ï¼ˆç”¨æˆ·å¯è‡ªå·±æ”¹åŠ¨è¾¹çš„æ„é€ æ–¹å¼ï¼‰
# 
# 2. é…ç½®åŒ–ç”Ÿæˆæ¨¡å‹ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥æ ¹æ®æ•™ç¨‹è¿›è¡Œå›¾ç¥ç»ç½‘ç»œçš„å®ç°ã€‚
# 
# 3. å¼€å§‹è®­ç»ƒ
# 
# 4. æ‰§è¡Œé¢„æµ‹å¹¶äº§ç”Ÿç»“æœæ–‡ä»¶
# 

# In[1]:


# å¦‚æœéœ€è¦è¿›è¡ŒæŒä¹…åŒ–å®‰è£…, éœ€è¦ä½¿ç”¨æŒä¹…åŒ–è·¯å¾„, å¦‚ä¸‹æ–¹ä»£ç ç¤ºä¾‹:
# If a persistence installation is required, 
# you need to use the persistence path as the following: 
get_ipython().system('mkdir /home/aistudio/external-libraries')
get_ipython().system('pip install pgl easydict -q -t /home/aistudio/external-libraries')


# In[2]:


# åŒæ—¶æ·»åŠ å¦‚ä¸‹ä»£ç , è¿™æ ·æ¯æ¬¡ç¯å¢ƒ(kernel)å¯åŠ¨çš„æ—¶å€™åªè¦è¿è¡Œä¸‹æ–¹ä»£ç å³å¯: 
# Also add the following code, 
# so that every time the environment (kernel) starts, 
# just run the following code: 
import sys 
sys.path.append('/home/aistudio/external-libraries')


# In[3]:


import pgl
import paddle
import paddle.fluid as fluid
import paddle.nn as nn
import numpy as np
import time
import pandas as pd


# In[4]:


from easydict import EasyDict as edict

config = {
    "model_name": "GraphSAGE",
    "num_class": 35,
    "num_layers": 2,
    "dropout": 0.5,
    "learning_rate": 0.01,
    "weight_decay": 0.0005,
    "edge_dropout": 0.00
}

config = edict(config)


# ## æ•°æ®åŠ è½½æ¨¡å—
# 
# è¿™é‡Œä¸»è¦æ˜¯ç”¨äºè¯»å–æ•°æ®é›†ï¼ŒåŒ…æ‹¬è¯»å–å›¾æ•°æ®æ„å›¾ï¼Œä»¥åŠè®­ç»ƒé›†çš„åˆ’åˆ†ã€‚

# In[5]:


from collections import namedtuple

Dataset = namedtuple("Dataset", 
               ["graph", "num_classes", "train_index",
                "train_label", "valid_index", "valid_label", "test_index", "node_feat", "edges", "node_label"])

def load_edges(num_nodes, self_loop=True, add_inverse_edge=True):
    # ä»æ•°æ®ä¸­è¯»å–è¾¹
    edges = pd.read_csv("work/edges.csv", header=None, names=["src", "dst"]).values

    if add_inverse_edge:
        edges = np.vstack([edges, edges[:, ::-1]])

    if self_loop:
        src = np.arange(0, num_nodes)
        dst = np.arange(0, num_nodes)
        self_loop = np.vstack([src, dst]).T
        edges = np.vstack([edges, self_loop])
    
    return edges

def load():
    # ä»æ•°æ®ä¸­è¯»å–ç‚¹ç‰¹å¾å’Œè¾¹ï¼Œä»¥åŠæ•°æ®åˆ’åˆ†
    node_feat = np.load("work/feat.npy")
    num_nodes = node_feat.shape[0]
    edges = load_edges(num_nodes=num_nodes, self_loop=True, add_inverse_edge=True)
    graph = pgl.graph.Graph(num_nodes=num_nodes, edges=edges, node_feat={"feat": node_feat})
    
    df = pd.read_csv("work/train.csv")
    node_index = df["nid"].values
    node_label = df["label"].values
    train_part = int(len(node_index) * 0.8)
    train_index = node_index[:train_part]
    train_label = node_label[:train_part]
    valid_index = node_index[train_part:]
    valid_label = node_label[train_part:]
    test_index = pd.read_csv("work/test.csv")["nid"].values
    dataset = Dataset(graph=graph, 
                    train_label=train_label,
                    train_index=train_index,
                    valid_index=valid_index,
                    valid_label=valid_label,
                    test_index=test_index, num_classes=35, node_feat = node_feat, edges = edges, node_label=node_label)
    return dataset


# In[6]:


dataset = load()

train_index = dataset.train_index
train_label = paddle.to_tensor(np.reshape(dataset.train_label, [-1 , 1]))
train_index = paddle.to_tensor(np.expand_dims(train_index, -1))

val_index = dataset.valid_index
val_label = paddle.to_tensor(np.reshape(dataset.valid_label, [-1, 1]))
val_index = paddle.to_tensor(np.expand_dims(val_index, -1))

test_index = dataset.test_index
test_index = paddle.to_tensor(np.expand_dims(test_index, -1))
test_label = paddle.to_tensor(np.zeros((len(test_index), 1), dtype="int64"))
num_class = dataset.num_classes


# ## ç»„ç½‘æ¨¡å—
# 
# è¿™é‡Œæ˜¯ç»„ç½‘æ¨¡å—ï¼Œç›®å‰å·²ç»æä¾›äº†ä¸€äº›é¢„å®šä¹‰çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬**GCN**, **GAT**, **APPNP**ç­‰ã€‚å¯ä»¥é€šè¿‡ç®€å•çš„é…ç½®ï¼Œè®¾å®šæ¨¡å‹çš„å±‚æ•°ï¼Œhidden_sizeç­‰ã€‚ä½ ä¹Ÿå¯ä»¥æ·±å…¥åˆ°model.pyé‡Œé¢ï¼Œå»å¥‡æ€å¦™æƒ³ï¼Œå†™è‡ªå·±çš„å›¾ç¥ç»ç½‘ç»œã€‚

# In[7]:


import pgl
from pgl.sampling import subgraph
from pgl.graph import Graph
import graphmodel_1
from graphmodel_1 import Model
import paddle
import paddle.nn as nn
import numpy as np
import time
 #ä½¿ç”¨CPU
#place = fluid.CPUPlace()
# ä½¿ç”¨GPU
place = fluid.CUDAPlace(0)
model = Model(config)
lr = 0.01
#lr = paddle.optimizer.lr.ExponentialDecay(learning_rate=config.get("learning_rate", 0.01), gamma=0.9, verbose=True)
optim = paddle.optimizer.Adam(learning_rate = lr, parameters = model.parameters())


# ## å¼€å§‹è®­ç»ƒè¿‡ç¨‹
# 
# å›¾ç¥ç»ç½‘ç»œé‡‡ç”¨FullBatchçš„è®­ç»ƒæ–¹å¼ï¼Œæ¯ä¸€æ­¥è®­ç»ƒå°±ä¼šæŠŠæ‰€æœ‰æ•´å¼ å›¾è®­ç»ƒæ ·æœ¬å…¨éƒ¨è®­ç»ƒä¸€éã€‚
# 
# 

# In[8]:


epoch = 200
# å°†å›¾æ•°æ®å˜æˆ feed_dict ç”¨äºä¼ å…¥Paddle Excecutor
criterion = paddle.nn.loss.CrossEntropyLoss()


edges = dataset.edges
graph = dataset.graph
graph.tensor()
for epoch in range(epoch):
    # Full Batch è®­ç»ƒ
    # è®¾å®šå›¾ä¸Šé¢é‚£äº›èŠ‚ç‚¹è¦è·å–
    # node_index: è®­ç»ƒèŠ‚ç‚¹çš„nid    
    # node_label: è®­ç»ƒèŠ‚ç‚¹å¯¹åº”çš„æ ‡ç­¾
    # input shape = [N, Cin]
    # output shape [N, Co]

    #pgl.sampling.subgraph(graph, nodes, eid=None, edges=None, with_node_feat=True, with_edge_feat=True)
    #g = subgraph(graph=graph, nodes=train_index, edges=edges)
    #g.tensor()
    
    pred = model(graph, graph.node_feat["feat"])
    print(pred.shape)
    pred = paddle.gather(pred, train_index)
    loss = criterion(pred, train_label)
    loss.backward()
    acc = paddle.metric.accuracy(input=pred, label=train_label, k=1)
    optim.step()
    optim.clear_grad()
    
    # Full Batch éªŒè¯
    # è®¾å®šå›¾ä¸Šé¢é‚£äº›èŠ‚ç‚¹è¦è·å–
    # node_index: è®­ç»ƒèŠ‚ç‚¹çš„nid    
    # node_label: è®­ç»ƒèŠ‚ç‚¹å¯¹åº”çš„æ ‡ç­¾
    #g = subgraph(graph=graph, nodes=val_index, edges=edges)
    #g.tensor()
    val_pred = model(graph, graph.node_feat["feat"])
    val_pred = paddle.gather(val_pred, val_index)
    val_acc = paddle.metric.accuracy(input=val_pred, label=val_label, k=1)
    print("Epoch", epoch, "Train Acc", acc, "Valid Acc", val_acc)



# ## å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
# 
# è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚é¢„æµ‹çš„æ—¶å€™ï¼Œç”±äºä¸çŸ¥é“æµ‹è¯•é›†åˆçš„æ ‡ç­¾ï¼Œæˆ‘ä»¬éšæ„ç»™ä¸€äº›æµ‹è¯•labelã€‚æœ€ç»ˆæˆ‘ä»¬è·å¾—æµ‹è¯•æ•°æ®çš„é¢„æµ‹ç»“æœã€‚
# 

# ## ä¿å­˜æ¨¡å‹å‚æ•°å‡†å¤‡Correct and smooth
# è¿™é‡Œæˆ‘ä»¬è°ƒç”¨paddleæä¾›çš„æ¥å£saveæ¥ä¿å­˜æ¨¡å‹å‚æ•°ä¸ºmodel_state_dictï¼Œç„¶åç”Ÿæˆé¢„æµ‹labelã€‚

# In[9]:


test_pred = model(graph, graph.node_feat["feat"])
test_pred = paddle.gather(test_pred, test_index)
test_pred = paddle.argmax(test_pred, axis=1)
test_index = np.array(test_index)
test_pred = np.array(test_pred)
submission = pd.DataFrame(data={
                            "nid": test_index.reshape(-1),
                            "label": test_pred.reshape(-1)
                        })
submission.to_csv("submission.csv", index=False)

#saving the state dict for smoothing final results 
paddle.save(model.state_dict(), "model_state_dict")




# ## Correct and Smoothéƒ¨åˆ†
# å¦‚æœæˆ‘ä»¬ä½¿ç”¨MLPï¼Œå°±éœ€è¦è°ƒç”¨Correctéƒ¨åˆ†ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨äº†GATå°±éœ€è¦è°ƒç”¨Smoothéƒ¨åˆ†ã€‚

# In[10]:


from correctandsmooth import LayerPropagation, CorrectAndSmooth

model_state_dict = paddle.load('model_state_dict')
model.load_dict(model_state_dict)
y_pred = model(graph, graph.node_feat['feat']) 

y_soft = nn.functional.softmax(y_pred)

cas = CorrectAndSmooth(50, 0.979, 'DAD', 50, 0.756, 'DAD', 20.)

mask_idx = paddle.concat([train_index, val_index])
node_label = paddle.to_tensor(np.reshape(dataset.node_label, [-1 , 1]))

mask_label = paddle.gather(node_label, mask_idx)
mask_label = paddle.nn.functional.one_hot(mask_label, num_classes=35)
y_soft = cas.smooth(graph, y_soft, mask_label, mask_idx)



# ## ç”Ÿæˆæäº¤æ–‡ä»¶
# 
# æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨pandasè½»æ¾ç”Ÿæˆæäº¤æ–‡ä»¶ï¼Œæœ€åä¸‹è½½ submission.csv æäº¤å°±å¥½äº†ã€‚

# In[11]:


pred = paddle.argmax(y_soft, axis=-1, keepdim=True)
test_index = paddle.to_tensor(test_index)
pred = paddle.gather(pred, test_index)
test_index = np.array(test_index)
pred = np.array(pred)

test_index = np.array(test_index)
pred = np.array(pred)
submission = pd.DataFrame(data={
                            "nid": test_index.reshape(-1),
                            "label": pred.reshape(-1)
                        })
submission.to_csv("submission_cs.csv", index=False)


# ### 2. One More Thing
# 
# å¦‚æœå¤§å®¶è¿˜æƒ³è¦åˆ«çš„å¥‡æ€å¦™æƒ³ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹è®ºæ–‡ï¼Œä»–ä»¬éƒ½åœ¨èŠ‚ç‚¹åˆ†ç±»ä¸Šæœ‰å¾ˆå¤§æå‡ã€‚
# 
# * Predict then Propagate: Graph Neural Networks meet Personalized PageRank (https://arxiv.org/abs/1810.05997)
# 
# * Simple and Deep Graph Convolutional Networks (https://arxiv.org/abs/2007.02133)
# 
# * Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification (https://arxiv.org/abs/2009.03509)
# 
# * Combining Label Propagation and Simple Models Out-performs Graph Neural Networks (https://arxiv.org/abs/2010.13993)
# 
# 
# å¤§å®¶ä¹Ÿå¯ä»¥çœ‹çœ‹githubçš„ [UniMPç®—æ³•](https://github.com/PaddlePaddle/PGL/tree/main/ogb_examples/nodeproppred/unimp) è¿™ä¸ªä¾‹å­ï¼Œé‡Œé¢æœ‰ç›¸ä¼¼çš„æ•°æ®é›†ï¼Œå¹¶ä¸”æœ€è¿‘ä¹Ÿæ˜¯SOTAæ•ˆæœï¼Œæœ‰å¸®åŠ©ğŸ‘æ¬¢è¿ç‚¹Star
